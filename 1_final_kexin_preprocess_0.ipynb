{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, we haven't split the train and test yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scispacy #https://allenai.github.io/scispacy/\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gw1107/.conda/envs/NYU_DL/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: load Note datasets\n",
    "# update these constants to run this script\n",
    "preprocessed_data_folder = './Preprocessed_Data/'\n",
    "if not os.path.exists(preprocessed_data_folder):\n",
    "    os.mkdir(preprocessed_data_folder)\n",
    "\n",
    "OUTPUT_DIR =  preprocessed_data_folder#this path will contain tokenized notes. This dir will be the input dir for create_pretrain_data.sh\n",
    "MIMIC_NOTES_FILE = './physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv' #this is the path to mimic data if you're reading from a csv. Else uncomment the code to read from database below\n",
    "\n",
    "df_notes = pd.read_csv(MIMIC_NOTES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Generate the cohort to pretrain on (here e.g. we only pretrian on XXXX) \n",
    "# select category/ all data\n",
    "list0 = ['Echo', 'ECG', 'Rehab Services']\n",
    "list1 = ['Nursing', 'Physician ']\n",
    "# this is done\n",
    "#list2 = ['Case Management ', 'Respiratory ', 'Nutrition', 'General', ]\n",
    "list3 = ['Consult']\n",
    "list4 = ['Radiology', 'Nursing/other']\n",
    "list5 = ['Pharmacy','Social Work']\n",
    "\n",
    "number = 0\n",
    "\n",
    "df_notes = df_notes[df_notes.CATEGORY.isin(list0)]\n",
    "\n",
    "\n",
    "\n",
    "# # IMPORTANT: if you fine tune on the same dataset that you use for pretrain, you need to preclude the fine-tune test admissions \n",
    "# df_test_ids = pd.read_csv('FINETUNE_DATA_PATH/test.csv').HADM_ID.unique()\n",
    "# df_notes_fold = df_notes[~df_notes.HADM_ID.isin(df_test_ids)]\n",
    "df_notes_fold = df_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Preprocessing\n",
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    \n",
    "    # remove, digits, spaces\n",
    "    y = y.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "    y = \" \".join(y.split())\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_notes): \n",
    "    df_notes['TEXT']=df_notes['TEXT'].fillna(' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\n',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\r',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(str.strip)\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.lower()\n",
    "\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(lambda x: preprocess1(x))\n",
    "    \n",
    "    return df_notes\n",
    "\n",
    "df_notes_fold = preprocessing(df_notes_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df_notes_fold[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting sentence boundaries\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc\n",
    "\n",
    "# STEP 4: Notes to Sentences\n",
    "#### NOTE THAT HERE we use instead of english\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()  # just the language with no model\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner'])\n",
    "nlp.add_pipe(sbd_component, before='parser')  \n",
    "\n",
    "# nlp praser may not work when there is only one token. In these cases, we just remove them as note that has length 1 usually is some random stuff\n",
    "\n",
    "def toSentence(x):\n",
    "    doc = nlp(x)\n",
    "    text=[]\n",
    "    try:\n",
    "        for sent in doc.sents:\n",
    "            st=str(sent).strip() \n",
    "            if len(st)<20:\n",
    "                #a lot of abbreviation is segmented as one line. But these are all describing the previous things\n",
    "                #so I attached it to the sentence before\n",
    "                if len(text)!=0:\n",
    "                    text[-1]=' '.join((text[-1],st))\n",
    "                else:\n",
    "                    text=[st]\n",
    "            else:\n",
    "                text.append((st))\n",
    "    except:\n",
    "        print(doc)\n",
    "    return text\n",
    "\n",
    "pretrain_sent=df_notes_fold['TEXT'].apply(lambda x: toSentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260276/260276 [00:01<00:00, 245623.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Create Pretraining File\n",
    "file=open(OUTPUT_DIR + 'clinical_sentences_category_{}.txt'.format(number),'w')\n",
    "pretrain_sent_value = pretrain_sent.values\n",
    "for i in tqdm(range(len(pretrain_sent))):\n",
    "    if len(pretrain_sent_value[i]) > 0:\n",
    "        # remove the one token note\n",
    "        note = pretrain_sent_value[i]\n",
    "        for sent in note:\n",
    "            file.write(sent+'\\n')\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Download the Implementations and Initial Checkpoint with the SentencePiece Model and Vocab \n",
    "# Github Repo:\n",
    "# XLNet: git clone https://github.com/zihangdai/xlnet.git\n",
    "# BERT: git clone https://github.com/google-research/bert.git\n",
    "# Model: \n",
    "# XLNet-Base: https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n",
    "# BERT-Base-Uncased: https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Generate Pretraining Tensorflow TF_Records\n",
    "\n",
    "# For Clinical BERT\n",
    "# cd to the git repo\n",
    "\n",
    "# Generate datasets for 128 max seq\n",
    "!python create_pretraining_data.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain.txt \\\n",
    "  --output_file=PRETRAIN_DATA_PATH/tf_examples_128.tfrecord \\\n",
    "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=3\n",
    "\n",
    "# Generate datasets for 512 max seq\n",
    "!python create_pretraining_data.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain.txt \\\n",
    "  --output_file=PRETRAIN_DATA_PATH/tf_examples_512.tfrecord \\\n",
    "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=512 \\\n",
    "  --max_predictions_per_seq=76 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=3\n",
    "\n",
    "# For Clinical XLNet\n",
    "\n",
    "!python data_utils.py \\\n",
    "    --bsz_per_host=6 \\\n",
    "    --num_core_per_host=1 \\\n",
    "    --seq_len=512 \\\n",
    "    --reuse_len=256 \\\n",
    "    --input_glob=/scratch/kh2383/MechVent/data/clinical_sentences_pretrain_xlnet.txt \\\n",
    "    --save_dir=/scratch/kh2383/MechVent/data/xlnet_tfrecord/ \\\n",
    "    --num_passes=5 \\\n",
    "    --bi_data=True \\\n",
    "    --sp_path=/scratch/kh2383/clibert/xlnet_cased_L-12_H-768_A-12/spiece.model \\\n",
    "    --mask_alpha=6 \\\n",
    "    --mask_beta=1 \\\n",
    "    --num_predict=85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Pretraining Use Original TF implementation\n",
    "\n",
    "# For Clinical BERT\n",
    "\n",
    "# First pretrain 100000 steps on the max seq length of 128\n",
    "!python run_pretraining.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/tf_examples_128.tfrecord \\\n",
    "  --output_dir=PRETRAINED_MODEL_PATH/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=INITIAL_DATA_PATH/bert_config.json \\\n",
    "  --init_checkpoint=INITIAL_DATA_PATH/bert_model.ckpt \\\n",
    "  --train_batch_size=64 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=100000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "\n",
    "# Then further pretrain 100000 steps on the max seq length of 512\n",
    "# NOTE: the init_checkpoint should switch to the 128 pretrained model\n",
    "\n",
    "!python run_pretraining.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/tf_examples_512.tfrecord \\\n",
    "  --output_dir=PRETRAINED_MODEL_PATH/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=INITIAL_DATA_PATH/bert_config.json \\\n",
    "  --init_checkpoint=PRETRAINED_MODEL_PATH/pretraining_output_128/model.ckpt-100000 \\\n",
    "  --train_batch_size=16 \\\n",
    "  --max_seq_length=512 \\\n",
    "  --max_predictions_per_seq=76 \\\n",
    "  --num_train_steps=100000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "\n",
    "\n",
    "# For Clinical XLNet\n",
    "\n",
    "# Pretrain for 200000 steps\n",
    "\n",
    "!python train_gpu.py \\\n",
    "    --record_info_dir=PRETRAIN_DATA_PATH/xlnet_tfrecord/tfrecords/ \\\n",
    "    --model_dir=PRETRAINED_MODEL_PATH/xlnet_model/ \\\n",
    "    --init_checkpoint=INITIAL_DATA_PATH/xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt \\\n",
    "    --train_batch_size=8 \\\n",
    "    --seq_len=512 \\\n",
    "    --reuse_len=256 \\\n",
    "    --mem_len=384 \\\n",
    "    --perm_size=256 \\\n",
    "    --n_layer=12 \\\n",
    "    --d_model=768 \\\n",
    "    --d_embed=768 \\\n",
    "    --n_head=12 \\\n",
    "    --d_head=64 \\\n",
    "    --d_inner=3072 \\\n",
    "    --untie_r=True \\\n",
    "    --mask_alpha=6 \\\n",
    "    --mask_beta=1 \\\n",
    "    --num_predict=85 \\\n",
    "    --num_hosts=1 \\\n",
    "    --num_core_per_host=2 \\\n",
    "    --train_steps=200000 \\\n",
    "    --iterations=500 \\\n",
    "    --save_steps=5000 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
